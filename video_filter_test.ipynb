{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è§†é¢‘è¿‡æ»¤å™¨å¯è§†åŒ–æµ‹è¯•\n",
    "\n",
    "è¿™ä¸ªNotebookç”¨äºå¯è§†åŒ–æµ‹è¯•å•ä¸ªè§†é¢‘æ–‡ä»¶çš„è¿‡æ»¤è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n",
    "1. åˆ†è¾¨ç‡æ£€æµ‹\n",
    "2. äººè„¸æ£€æµ‹å’Œå¯è§†åŒ–\n",
    "3. FaceXFormerå¹´é¾„ä¼°è®¡\n",
    "4. æœ€ç»ˆè¿‡æ»¤ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "from simple_video_filter import SimpleVideoFilter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ“ åº“å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®æµ‹è¯•å‚æ•°\n",
    "VIDEO_PATH = \"rawdata/test/111.mp4\"  # ä¿®æ”¹è¿™é‡Œæ¥æµ‹è¯•ä¸åŒçš„è§†é¢‘\n",
    "SAMPLE_FRAMES = 5  # é‡‡æ ·å¸§æ•°\n",
    "MAX_DISPLAY_FRAMES = 3  # æœ€å¤šæ˜¾ç¤ºçš„å¸§æ•°\n",
    "\n",
    "print(f\"æµ‹è¯•è§†é¢‘: {VIDEO_PATH}\")\n",
    "print(f\"é‡‡æ ·å¸§æ•°: {SAMPLE_FRAMES}\")\n",
    "print(f\"æ˜¾ç¤ºå¸§æ•°: {MAX_DISPLAY_FRAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–è§†é¢‘è¿‡æ»¤å™¨\n",
    "print(\"æ­£åœ¨åˆå§‹åŒ–FaceXFormerè§†é¢‘è¿‡æ»¤å™¨...\")\n",
    "filter_tool = SimpleVideoFilter(use_facexformer=True)\n",
    "print(\"âœ“ è¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è§†é¢‘åŸºæœ¬ä¿¡æ¯æ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æµ‹è§†é¢‘åŸºæœ¬ä¿¡æ¯\n",
    "def get_video_info(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    \n",
    "    info = {\n",
    "        'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "        'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "        'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "        'duration': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / cap.get(cv2.CAP_PROP_FPS)\n",
    "    }\n",
    "    cap.release()\n",
    "    return info\n",
    "\n",
    "# è·å–è§†é¢‘ä¿¡æ¯\n",
    "video_info = get_video_info(VIDEO_PATH)\n",
    "\n",
    "if video_info:\n",
    "    print(\"ğŸ“¹ è§†é¢‘åŸºæœ¬ä¿¡æ¯:\")\n",
    "    print(f\"  åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}\")\n",
    "    print(f\"  å¸§ç‡: {video_info['fps']:.2f} FPS\")\n",
    "    print(f\"  æ€»å¸§æ•°: {video_info['frame_count']}\")\n",
    "    print(f\"  æ—¶é•¿: {video_info['duration']:.2f} ç§’\")\n",
    "    \n",
    "    # åˆ†è¾¨ç‡æ£€æµ‹\n",
    "    resolution_passed = video_info['width'] >= 720 and video_info['height'] >= 1080\n",
    "    print(f\"\\nğŸ” åˆ†è¾¨ç‡æ£€æµ‹: {'âœ“ é€šè¿‡' if resolution_passed else 'âœ— ä¸é€šè¿‡'} (è¦æ±‚: â‰¥720x1080)\")\n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. äººè„¸æ£€æµ‹å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–å’Œæ˜¾ç¤ºé‡‡æ ·å¸§\n",
    "def extract_sample_frames(video_path, num_frames=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, min(num_frames, total_frames), dtype=int)\n",
    "    \n",
    "    frames = []\n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append((frame_idx, frame))\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# äººè„¸æ£€æµ‹å’Œå¯è§†åŒ–\n",
    "def detect_and_visualize_faces(frame, face_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    # åœ¨å›¾åƒä¸Šç»˜åˆ¶äººè„¸æ¡†\n",
    "    frame_with_faces = frame.copy()\n",
    "    valid_faces = 0\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        if w >= 256 and h >= 256:\n",
    "            # æœ‰æ•ˆäººè„¸ - ç»¿è‰²æ¡†\n",
    "            cv2.rectangle(frame_with_faces, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "            cv2.putText(frame_with_faces, f'Valid: {w}x{h}', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            valid_faces += 1\n",
    "        else:\n",
    "            # æ— æ•ˆäººè„¸ - çº¢è‰²æ¡†\n",
    "            cv2.rectangle(frame_with_faces, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.putText(frame_with_faces, f'Small: {w}x{h}', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    \n",
    "    return frame_with_faces, len(faces), valid_faces\n",
    "\n",
    "# æå–é‡‡æ ·å¸§\n",
    "sample_frames = extract_sample_frames(VIDEO_PATH, SAMPLE_FRAMES)\n",
    "print(f\"ğŸ“¸ æå–äº† {len(sample_frames)} ä¸ªé‡‡æ ·å¸§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºäººè„¸æ£€æµ‹ç»“æœ\n",
    "if sample_frames and filter_tool.face_cascade:\n",
    "    fig, axes = plt.subplots(min(len(sample_frames), MAX_DISPLAY_FRAMES), 1, \n",
    "                            figsize=(12, 4*min(len(sample_frames), MAX_DISPLAY_FRAMES)))\n",
    "    \n",
    "    if min(len(sample_frames), MAX_DISPLAY_FRAMES) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    total_faces = 0\n",
    "    total_valid_faces = 0\n",
    "    \n",
    "    for i, (frame_idx, frame) in enumerate(sample_frames[:MAX_DISPLAY_FRAMES]):\n",
    "        frame_with_faces, face_count, valid_face_count = detect_and_visualize_faces(\n",
    "            frame, filter_tool.face_cascade)\n",
    "        \n",
    "        total_faces += face_count\n",
    "        total_valid_faces += valid_face_count\n",
    "        \n",
    "        # è½¬æ¢BGRåˆ°RGBç”¨äºmatplotlibæ˜¾ç¤º\n",
    "        frame_rgb = cv2.cvtColor(frame_with_faces, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[i].imshow(frame_rgb)\n",
    "        axes[i].set_title(f'Frame {frame_idx}: {face_count} faces detected, {valid_face_count} valid (â‰¥256x256)')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # äººè„¸æ£€æµ‹ç»Ÿè®¡\n",
    "    face_passed = total_valid_faces > 0\n",
    "    print(f\"\\nğŸ‘¥ äººè„¸æ£€æµ‹ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»æ£€æµ‹äººè„¸æ•°: {total_faces}\")\n",
    "    print(f\"  æœ‰æ•ˆäººè„¸æ•°: {total_valid_faces} (â‰¥256x256)\")\n",
    "    print(f\"  äººè„¸æ£€æµ‹ç»“æœ: {'âœ“ é€šè¿‡' if face_passed else 'âœ— ä¸é€šè¿‡'}\")\n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¿›è¡Œäººè„¸æ£€æµ‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FaceXFormerå¹´é¾„ä¼°è®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¹´é¾„ä¼°è®¡å¯è§†åŒ–\n",
    "def visualize_age_estimation(video_path, filter_tool):\n",
    "    print(\"ğŸ§  æ­£åœ¨è¿›è¡ŒFaceXFormerå¹´é¾„ä¼°è®¡...\")\n",
    "    \n",
    "    # ä½¿ç”¨è¿‡æ»¤å™¨çš„å¹´é¾„æ£€æµ‹åŠŸèƒ½\n",
    "    age_passed, age_info = filter_tool.check_age(video_path, sample_frames=5)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å¹´é¾„ä¼°è®¡ç»“æœ:\")\n",
    "    print(f\"  ä¼°è®¡å¹´é¾„: {age_info.get('estimated_age', 'N/A')}\")\n",
    "    print(f\"  å¹´é¾„ç±»åˆ«: {age_info.get('age_class', 'N/A')}\")\n",
    "    print(f\"  æ£€æµ‹æ–¹æ³•: {age_info.get('method', 'N/A')}\")\n",
    "    print(f\"  ç½®ä¿¡åº¦: {age_info.get('confidence', 'N/A')}\")\n",
    "    print(f\"  å¤„ç†äººè„¸æ•°: {age_info.get('face_count', 'N/A')}\")\n",
    "    \n",
    "    if 'age_distribution' in age_info:\n",
    "        print(f\"  å¹´é¾„åˆ†å¸ƒ: {age_info['age_distribution']}\")\n",
    "    \n",
    "    print(f\"  å¹´é¾„æ£€æµ‹ç»“æœ: {'âœ“ é€šè¿‡' if age_passed else 'âœ— ä¸é€šè¿‡'} (è¦æ±‚: 0-30å²)\")\n",
    "    \n",
    "    return age_passed, age_info\n",
    "\n",
    "# æ‰§è¡Œå¹´é¾„ä¼°è®¡\n",
    "if os.path.exists(VIDEO_PATH):\n",
    "    age_passed, age_info = visualize_age_estimation(VIDEO_PATH, filter_tool)\n",
    "else:\n",
    "    print(\"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "    age_passed = False\n",
    "    age_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å¹´é¾„åˆ†ç±»å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¹´é¾„åˆ†ç±»å›¾è¡¨\n",
    "def plot_age_classification():\n",
    "    age_classes = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '70+']\n",
    "    pass_status = ['é€šè¿‡', 'é€šè¿‡', 'é€šè¿‡', 'ä¸é€šè¿‡', 'ä¸é€šè¿‡', 'ä¸é€šè¿‡', 'ä¸é€šè¿‡', 'ä¸é€šè¿‡']\n",
    "    colors = ['green' if status == 'é€šè¿‡' else 'red' for status in pass_status]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    bars = ax.bar(age_classes, [1]*8, color=colors, alpha=0.7)\n",
    "    \n",
    "    # æ ‡è®°å½“å‰æ£€æµ‹ç»“æœ\n",
    "    if age_info and 'estimated_age' in age_info:\n",
    "        current_age = age_info['estimated_age']\n",
    "        if current_age in age_classes:\n",
    "            idx = age_classes.index(current_age)\n",
    "            bars[idx].set_edgecolor('blue')\n",
    "            bars[idx].set_linewidth(4)\n",
    "            ax.text(idx, 0.5, f'å½“å‰æ£€æµ‹\\n{current_age}', ha='center', va='center', \n",
    "                   fontsize=12, fontweight='bold', color='blue')\n",
    "    \n",
    "    ax.set_title('FaceXFormerå¹´é¾„åˆ†ç±»ç³»ç»Ÿ (ç»¿è‰²=é€šè¿‡è¿‡æ»¤, çº¢è‰²=ä¸é€šè¿‡, è“è‰²è¾¹æ¡†=å½“å‰æ£€æµ‹ç»“æœ)', fontsize=14)\n",
    "    ax.set_xlabel('å¹´é¾„èŒƒå›´', fontsize=12)\n",
    "    ax.set_ylabel('è¿‡æ»¤çŠ¶æ€', fontsize=12)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    \n",
    "    # æ·»åŠ å›¾ä¾‹\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='green', alpha=0.7, label='é€šè¿‡ (0-30å²)'),\n",
    "                      Patch(facecolor='red', alpha=0.7, label='ä¸é€šè¿‡ (31å²ä»¥ä¸Š)')]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_age_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æœ€ç»ˆè¿‡æ»¤ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„è¿‡æ»¤æµ‹è¯•\n",
    "print(\"ğŸ¯ æ‰§è¡Œå®Œæ•´çš„è§†é¢‘è¿‡æ»¤æµ‹è¯•...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(VIDEO_PATH):\n",
    "    result = filter_tool.process_video(VIDEO_PATH)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ æœ€ç»ˆè¿‡æ»¤ç»“æœ:\")\n",
    "    print(f\"  è§†é¢‘æ–‡ä»¶: {result['video_name']}\")\n",
    "    print(f\"  åˆ†è¾¨ç‡æ£€æµ‹: {'âœ“ é€šè¿‡' if result['resolution_passed'] else 'âœ— ä¸é€šè¿‡'} ({result['resolution']})\")\n",
    "    print(f\"  äººè„¸æ£€æµ‹: {'âœ“ é€šè¿‡' if result['face_passed'] else 'âœ— ä¸é€šè¿‡'} ({result['face_info']})\")\n",
    "    print(f\"  å¹´é¾„æ£€æµ‹: {'âœ“ é€šè¿‡' if result['age_passed'] else 'âœ— ä¸é€šè¿‡'} ({result['age_info']})\")\n",
    "    print(f\"  æœ€ç»ˆç»“æœ: {'ğŸ‰ é€šè¿‡æ‰€æœ‰è¿‡æ»¤æ¡ä»¶' if result['final_passed'] else 'âŒ æœªé€šè¿‡è¿‡æ»¤'}\")\n",
    "    \n",
    "    # ç»“æœå¯è§†åŒ–\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    criteria = ['åˆ†è¾¨ç‡\\n(â‰¥720x1080)', 'äººè„¸æ£€æµ‹\\n(â‰¥256x256)', 'å¹´é¾„ä¼°è®¡\\n(0-30å²)', 'æœ€ç»ˆç»“æœ']\n",
    "    results = [result['resolution_passed'], result['face_passed'], \n",
    "              result['age_passed'], result['final_passed']]\n",
    "    colors = ['green' if r else 'red' for r in results]\n",
    "    \n",
    "    bars = ax.bar(criteria, [1]*4, color=colors, alpha=0.7)\n",
    "    \n",
    "    # æ·»åŠ ç»“æœæ ‡ç­¾\n",
    "    for i, (bar, passed) in enumerate(zip(bars, results)):\n",
    "        ax.text(i, 0.5, 'âœ“ é€šè¿‡' if passed else 'âœ— ä¸é€šè¿‡', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.set_title(f'è§†é¢‘è¿‡æ»¤ç»“æœæ€»è§ˆ: {VIDEO_PATH}', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('è¿‡æ»¤çŠ¶æ€', fontsize=12)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    \n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æµ‹è¯•å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
